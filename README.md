<div align="center">
<h1>
 DevOpsPal
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co" target="_blank">DevOpsPal-7B-Base</a> 
  â€¢ 
ğŸ¤— <a href="https://huggingface.co" target="_blank">DevOpsPal-7B-Chat</a> 
</p>

<div align="center">
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/luban-agi/DevOpsPal/main/README.md">English</a>
    <p>
</h4>
</div>

# æœ€æ–°
- [2023.9.30] ......

<br>
<br>
<br>

# ä»‹ç»
DevOpsPal æ˜¯ä¸€ç³»åˆ—åŸºäº DevOps çŸ¥è¯†çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºäº†å¸®åŠ©å·¥ç¨‹å¸ˆåœ¨ DevOps æ•´ä¸ªç”Ÿå‘½å‘¨æœŸåšåˆ°æœ‰é—®é¢˜ï¼Œé—® DevOpsPalã€‚

DevOps å°†æ•´ä¸ªé¡¹ç›®ç”Ÿå‘½å‘¨æœŸåˆ’åˆ†ä¸ºäº†ä¸ƒä¸ªé˜¶æ®µï¼Œåˆ†åˆ«ä¸ºï¼šè®¡åˆ’ï¼Œç¼–ç ï¼Œæ„å»ºï¼Œæµ‹è¯•ï¼Œéƒ¨ç½²ï¼Œè¿ç»´ï¼Œè§‚å¯Ÿã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ•´ä¸ªå‘¨æœŸå±äºä¸€ä¸ªæ— é™çš„å¾ªç¯ã€‚
<br>
<div  align="center">
 <img src="https://github.com/luban-agi/DevOpsPal/blob/main/image/devops_flow.png" width = "700" height = "350" alt="devops flow" align=center />
</div>

åœ¨è¿™ä¸ªå¾ªç¯ä¸­ï¼Œæ¯ä¸€æ­¥åœ¨å®æ–½çš„æ—¶å€™ï¼Œéƒ½ä¼šäº§ç”Ÿå„ç§çš„é—®é¢˜éœ€è¦å»æœå¯»ç­”æ¡ˆï¼Œæ¯”å¦‚åœ¨æ„å»ºè¿‡ç¨‹ä¸­ï¼Œéœ€è¦äº†è§£æŸä¸ªåŒ…çš„å‡½æ•°ã€‚ä»¥å¾€ä¼šä¸»è¦ä»ç½‘ç»œä¸Šæœç´¢ç›¸å…³çš„ç­”æ¡ˆï¼Œè¿™ä¸€æ­¥ä¼šæ¯”è¾ƒè€—æ—¶ï¼Œè€Œä¸”å¯èƒ½è¿˜æ‰¾ä¸åˆ°æƒ³è¦çš„ç»“æœã€‚

æ‰€ä»¥æˆ‘ä»¬åŸºäºè‡ªå·±æ”¶é›†çš„ DevOps çš„ç›¸å…³æ•°æ®ï¼Œäº§å‡ºäº†é¦–ä¸ªä»¥å¸®åŠ©å·¥ç¨‹å¸ˆåœ¨æ•´ä¸ª DevOps ç”Ÿå‘½å‘¨æœŸä¸­å¯èƒ½é‡åˆ°çš„é—®é¢˜ä¸ºç›®çš„çš„å¤§æ¨¡å‹ï¼Œå–åä¸º DevOpsPalã€‚æˆ‘ä»¬åˆ†åˆ«å¼€æºäº†ç»è¿‡ DevOps è¯­æ–™åŠ è®­è¿‡çš„ Base æ¨¡å‹å’Œç»è¿‡ DevOps QA æ•°æ®å¯¹é½è¿‡åçš„ Chat æ¨¡å‹ã€‚åœ¨ DevOps çš„è¯„æµ‹æ¦œå•ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†åŒå‚æ•°é‡çº§åˆ« SOTA çš„æ°´å¹³ã€‚ç›¸å…³è®­ç»ƒæ•°æ®é›†å’Œè¯„æµ‹æ•°æ®é›†ä¹Ÿå·²ç»å¼€æºã€‚

<br>
<br>
<br>


# æ¨¡å‹

| **æ¨¡å‹å** | **å‚æ•°é‡** | **è®­ç»ƒæ•°æ®** | **åŸºç¡€æ¨¡å‹** | **åœ°å€** |
| :--- | :---- |:----| :---- | :----| 
| DevOpsPal-7B | 7B | DevOps Corpus|Qwen-7B | Coming soon|
| DevOpsPal-7B-Chat | 7B | DevOps Corpus + DevOps QA| Qwen-7B | Coming soon|
| DevOpsPal-7B-Chat-int4 | 7B | DevOps Corpus + DevOps QA| Qwen-7B | Coming soon|
| DevOpsPal-13B | 13B | DevOps Corpus| Baichuan-13B | Coming soon|
| DevOpsPal-13B-Chat | 13B | DevOps Corpus + DevOps QA| Baichuan-13B | Coming soon|
| DevOpsPal-13B-Chat-int4 | 13B | DevOps Corpus + DevOps QA| Baichuan-13B | Coming soon|

<br>
<br>
<br>



# è¯„æµ‹
Coming soon

<br>
<br>
<br>

# ä½¿ç”¨
## æ¨ç†
é’ˆå¯¹ DevOpsPal-7B-Chatï¼Œå¯ä»¥ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œæ¨ç†
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚
tokenizer = AutoTokenizer.from_pretrained("path_to_DevOpsPal-7B-Chat", trust_remote_code=True)

# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("path_to_DevOpsPal-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("path_to_DevOpsPal-7B-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, "ã€‚ã€‚ã€‚", history=history)
print(response)

# ç¬¬ä¸‰è½®å¯¹è¯
response, history = model.chat(tokenizer, "ã€‚ã€‚ã€‚", history=history)
print(response)

```



<br>
<br>
<br>

## å¾®è°ƒ
æ•´ä½“è®­ç»ƒæ¡†æ¶æ˜¯åŸºäº LLaMA-Efficient-Tuning å¼€æºåº“ä¿®æ”¹è€Œæ¥ï¼Œæ„Ÿè°¢ä½œè€…çš„ä»˜å‡ºã€‚æ›´åŠ è¯¦ç»†çš„æ–‡æ¡£å¯ä»¥ç§»æ­¥  [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning/tree/main) ã€‚

<br>

### æ•°æ®å‡†å¤‡
ä»£ç å†…éƒ¨é€šè¿‡è°ƒç”¨ datasets.load_dataset è¯»å–æ•°æ®ï¼Œæ”¯æŒ load_dataset æ‰€æ”¯æŒçš„æ•°æ®è¯»å–æ–¹å¼ï¼Œæ¯”å¦‚ jsonï¼Œcsvï¼Œè‡ªå®šä¹‰è¯»å–è„šæœ¬ç­‰æ–¹å¼ï¼ˆä½†æ¨èæ•°æ®å‡†å¤‡ä¸º jsonl æ ¼å¼çš„æ–‡ä»¶ï¼‰ã€‚ç„¶åè¿˜éœ€è¦æ›´æ–° `data/dataset_info.json` æ–‡ä»¶ï¼Œå…·ä½“å¯ä»¥å‚è€ƒ `data/README.md`ã€‚

<br>

### Pretrain
å¦‚æœæ”¶é›†äº†ä¸€æ‰¹æ–‡æ¡£ä¹‹ç±»çš„è¯­æ–™ï¼ˆæ¯”å¦‚å…¬å¸å†…éƒ¨äº§å“çš„æ–‡æ¡£ï¼‰æƒ³è¦åœ¨ devopspal æ¨¡å‹ä¸ŠåŠ è®­ï¼Œå¯ä»¥æ‰§è¡Œ `scripts/devopspal-pt.sh` æ¥å‘èµ·ä¸€æ¬¡åŠ è®­æ¥è®©æ¨¡å‹å­¦ä¹ åˆ°è¿™æ‰¹æ–‡æ¡£çš„çŸ¥è¯†ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹:

```bash
set -v 

torchrun --nproc_per_node=4 --nnodes=$WORLD_SIZE --master_port=$MASTER_PORT --master_addr=$MASTER_ADDR --node_rank=$RANK src/train_bash.py \
    --deepspeed conf/deepspeed_config.json \    # deepspeed é…ç½®åœ°å€
	--stage pt \    # ä»£è¡¨æ‰§è¡Œ pretrain
    --model_name_or_path path_to_model \    # huggingfaceä¸‹è½½çš„ devopspal æ¨¡å‹åœ°å€
    --do_train \
    --report_to 'tensorboard' \
    --dataset your_corpus \    # æ•°æ®é›†åå­—ï¼Œè¦å’Œåœ¨ dataset_info.json ä¸­å®šä¹‰çš„ä¸€è‡´
    --template default \    # templateï¼Œpretrain å°±æ˜¯ default
    --finetuning_type full \  # å…¨é‡æˆ–è€… lora
    --output_dir path_to_output_checkpoint_path \    # æ¨¡å‹ checkpoint ä¿å­˜çš„è·¯å¾„
    --overwrite_cache \
    --per_device_train_batch_size 8 \    
    --per_device_eval_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.05 \
    --evaluation_strategy steps \
    --logging_steps 10 \
    --max_steps 1000 \
    --save_steps 1000 \
    --eval_steps 1000 \
    --learning_rate 5e-6 \
    --plot_loss \
    --max_source_length=2048 \
    --dataloader_num_workers 8 \
    --val_size 0.01 \
    --bf16 \
    --overwrite_output_dir
```

ä½¿ç”¨è€…å¯ä»¥åœ¨è¿™ä¸ªåŸºç¡€ä¸Šè°ƒæ•´æ¥å‘èµ·è‡ªå·±çš„è®­ç»ƒï¼Œæ›´åŠ è¯¦ç»†çš„å¯é…ç½®é¡¹å»ºè®®é€šè¿‡ `python src/train_bash.py -h` æ¥è·å–å®Œæ•´çš„å‚æ•°åˆ—è¡¨ã€‚

<br>

### SFT
å¦‚æœæ”¶é›†äº†ä¸€æ‰¹ QA æ•°æ®æƒ³è¦é’ˆå¯¹ devopspal å†è¿›è¡Œå¯¹é½çš„è¯ï¼Œå¯ä»¥æ‰§è¡Œ `scripts/devopspal-sft.sh` æ¥å‘èµ·ä¸€æ¬¡åŠ è®­æ¥è®©æ¨¡å‹åœ¨æ”¶é›†åˆ°çš„æ¨¡å‹ä¸Šè¿›è¡Œå¯¹é½ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹:
```bash
set -v 

torchrun --nproc_per_node=2 --nnodes=$WORLD_SIZE --master_port=$MASTER_PORT --master_addr=$MASTER_ADDR --node_rank=$RANK src/train_bash.py \
    --deepspeed conf/deepspeed_config.json \    # deepspeed é…ç½®åœ°å€
	--stage pt \    # ä»£è¡¨æ‰§è¡Œ pretrain
    --model_name_or_path path_to_model \    # huggingfaceä¸‹è½½çš„æ¨¡å‹åœ°å€
    --do_train \
    --report_to 'tensorboard' \
    --dataset your_corpus \    # æ•°æ®é›†åå­—ï¼Œè¦å’Œåœ¨ dataset_info.json ä¸­å®šä¹‰çš„ä¸€è‡´
    --template chatml \    # template qwen æ¨¡å‹å›ºå®šå†™ chatml
    --finetuning_type full \    # å…¨é‡æˆ–è€… lora
    --output_dir /mnt/llm/devopspal/model/trained \     # æ¨¡å‹ checkpoint ä¿å­˜çš„è·¯å¾„
    --overwrite_cache \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --gradient_accumulation_steps 1 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.05 \
    --evaluation_strategy steps \
    --logging_steps 10 \
    --max_steps 1000 \
    --save_steps 100 \
    --eval_steps 100 \
    --learning_rate 5e-5 \
    --plot_loss \
    --max_source_length=2048 \
    --dataloader_num_workers 8 \
    --val_size 0.01 \
    --bf16 \
    --overwrite_output_dir
```

ä½¿ç”¨è€…å¯ä»¥åœ¨è¿™ä¸ªåŸºç¡€ä¸Šè°ƒæ•´æ¥å‘èµ·è‡ªå·±çš„ SFT è®­ç»ƒï¼Œæ›´åŠ è¯¦ç»†çš„å¯é…ç½®é¡¹å»ºè®®é€šè¿‡ `python src/train_bash.py -h` æ¥è·å–å®Œæ•´çš„å‚æ•°åˆ—è¡¨ã€‚


<br>
<br>
<br>

## é‡åŒ–
æˆ‘ä»¬æä¾›äº† DevOpsPal-Chat ç³»åˆ—çš„é‡åŒ–ç‰ˆæœ¬æ¨¡å‹ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ¥é‡åŒ–è‡ªå·±åŠ è®­è¿‡çš„æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from optimum.gptq import GPTQQuantizer, load_quantized_model
import torch

# åŠ è½½æ¨¡å‹
model_name = "path_of_your_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

# åŠ è½½æ•°æ®
# todo

# å¼€å§‹é‡åŒ–
quantizer = GPTQQuantizer(bits=4, dataset="c4", block_name_to_quantize = "model.decoder.layers", model_seqlen = 2048)
quantized_model = quantizer.quantize_model(model, tokenizer)

# ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
out_dir = 'save_path_of_your_quantized_model'
quantized_model.save_quantized(out_dir)
```

æ¨èé˜…è¯» [Quantization](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization) æ¥è·å–æ›´å¤šçš„å¸®åŠ©ã€‚


<br>
<br>
<br>

# æ•°æ®
## DevOps Corpus
Coming soon

<br>
<br>
<br>

## DevOps QA
Coming soon

<br>
<br>
<br>


# å…è´£å£°æ˜
ç”±äºè¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ¨¡å‹ç”Ÿæˆçš„å†…å®¹å¯èƒ½åŒ…å«å¹»è§‰æˆ–è€…æ­§è§†æ€§è¨€è®ºã€‚è¯·è°¨æ…ä½¿ç”¨ DevOpsPal ç³»åˆ—æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ã€‚
å¦‚æœè¦å…¬å¼€ä½¿ç”¨æˆ–å•†ç”¨è¯¥æ¨¡å‹æœåŠ¡ï¼Œè¯·æ³¨æ„æœåŠ¡æ–¹éœ€æ‰¿æ‹…ç”±æ­¤äº§ç”Ÿçš„ä¸è‰¯å½±å“æˆ–æœ‰å®³è¨€è®ºçš„è´£ä»»ï¼Œæœ¬é¡¹ç›®å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•ç”±ä½¿ç”¨æœ¬é¡¹ç›®ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®ã€æ¨¡å‹ã€ä»£ç ç­‰ï¼‰å¯¼è‡´çš„å±å®³æˆ–æŸå¤±ã€‚

<br>
<br>
<br>

# Star History
[![Star History Chart](https://api.star-history.com/svg?repos=luban-agi/DevOpsPal&type=Date)](https://star-history.com/#luban-agi/DevOpsPal&Date)
